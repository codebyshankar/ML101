{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2235268,"sourceType":"datasetVersion","datasetId":1342999}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nhousing_file = \"housing.csv\"\nhousing_file_path = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.find(housing_file) != -1:\n            housing_file_path = os.path.join(dirname, filename)\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T08:47:12.000376Z","iopub.execute_input":"2024-04-21T08:47:12.000747Z","iopub.status.idle":"2024-04-21T08:47:13.196662Z","shell.execute_reply.started":"2024-04-21T08:47:12.000715Z","shell.execute_reply":"2024-04-21T08:47:13.195051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data preprocessing\n\n### 1.1 Let us load the housing dataset and get some intuition\n\n__*Following implementation is based on the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow 3rd Edition by Aurélien Géron*__","metadata":{}},{"cell_type":"code","source":"housing = pd.read_csv(housing_file_path)\n\n# print(housing.info())\n# print(housing.head())\n# print(housing.describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:13.198870Z","iopub.execute_input":"2024-04-21T08:47:13.199304Z","iopub.status.idle":"2024-04-21T08:47:13.328625Z","shell.execute_reply.started":"2024-04-21T08:47:13.199275Z","shell.execute_reply":"2024-04-21T08:47:13.327233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Find how many categories of ocean_proximity are there and how many samples in each category","metadata":{}},{"cell_type":"code","source":"print(housing[\"ocean_proximity\"].value_counts()) # need to convert this categorical column to numeric equivalent","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:13.330224Z","iopub.execute_input":"2024-04-21T08:47:13.330899Z","iopub.status.idle":"2024-04-21T08:47:13.340890Z","shell.execute_reply.started":"2024-04-21T08:47:13.330864Z","shell.execute_reply":"2024-04-21T08:47:13.338789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(12, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:13.343269Z","iopub.execute_input":"2024-04-21T08:47:13.343892Z","iopub.status.idle":"2024-04-21T08:47:15.060227Z","shell.execute_reply.started":"2024-04-21T08:47:13.343859Z","shell.execute_reply":"2024-04-21T08:47:15.058968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Let us get test data out of the way before we build any further intuition\n\n#### 1.3.1 Let us process median_income - It should have more impact on the label (median_house_value)\n\nIn oder to get test data that properly represents different income categories, instead of random split to get some test data, let us do the following based on histogram of median_income.\n\nmedian_income can be split into a few ranges and then name each one as one category (like 1, 2, 3...).\n\nWe can, infact, make it as a temporary column and use it to select test data from each of the income category, so we have proportional test data from each income category.","metadata":{}},{"cell_type":"code","source":"housing['income_category'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3.0, 4.5, 6.0, np.inf], labels=[1, 2, 3, 4, 5])\nhousing['income_category'].value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.xlabel(\"Income category\")\nplt.ylabel(\"Number of districts\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:15.062643Z","iopub.execute_input":"2024-04-21T08:47:15.062983Z","iopub.status.idle":"2024-04-21T08:47:15.249741Z","shell.execute_reply.started":"2024-04-21T08:47:15.062956Z","shell.execute_reply":"2024-04-21T08:47:15.248190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3.2 Use stratified split to get proper distribution of data set into train and test\n\nManually splitting is one option, gives better understanding of the underlying process...\nHowever, if that understanding is already gained, then we can just use train_test_split with stratify on income_category","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nstrat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, stratify=housing['income_category'], random_state=42)\n\n# let us see how well stratification worked\nprint(strat_test_set['income_category'].value_counts() / len(strat_test_set))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:15.251021Z","iopub.execute_input":"2024-04-21T08:47:15.251454Z","iopub.status.idle":"2024-04-21T08:47:16.936622Z","shell.execute_reply.started":"2024-04-21T08:47:15.251417Z","shell.execute_reply":"2024-04-21T08:47:16.935605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from now on we don't need income_category... let us drop it\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_category', axis=1, inplace=True)\n    \n# make strat_train_set as housing (now, housing will only be with train data (without test data, i.e. strat_test_data))\nhousing = strat_train_set.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:16.937737Z","iopub.execute_input":"2024-04-21T08:47:16.938015Z","iopub.status.idle":"2024-04-21T08:47:16.948627Z","shell.execute_reply.started":"2024-04-21T08:47:16.937992Z","shell.execute_reply":"2024-04-21T08:47:16.946822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3.3 Let us get more intuition on the data (without strat_test_data)\n\nLet us plot median_house_value (label to predict), population against longitude and latitude.\n\nThen we will see how median_house_value correlates with the features","metadata":{}},{"cell_type":"code","source":"housing.plot(kind='scatter', x=\"longitude\", y='latitude', grid=True,\n            s=housing['population'] / 100, label='population',\n            c='median_house_value', cmap='jet', colorbar=True,\n            legend=True, sharex=False, figsize=(12,8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:16.950328Z","iopub.execute_input":"2024-04-21T08:47:16.950807Z","iopub.status.idle":"2024-04-21T08:47:17.541902Z","shell.execute_reply.started":"2024-04-21T08:47:16.950777Z","shell.execute_reply":"2024-04-21T08:47:17.540798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot, median_house_value is high near south west, perhaps, near coastal area","metadata":{}},{"cell_type":"code","source":"corr_matrix = housing.corr(numeric_only=True) # only checks for linear correlation...\nprint(corr_matrix['median_house_value'].sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:17.543352Z","iopub.execute_input":"2024-04-21T08:47:17.543937Z","iopub.status.idle":"2024-04-21T08:47:17.556898Z","shell.execute_reply.started":"2024-04-21T08:47:17.543899Z","shell.execute_reply":"2024-04-21T08:47:17.555592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above correlation matrix value, median_income has the highest correlation (0.687151) with median_house_value","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\nscatter_matrix(housing[attributes], figsize=(12, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:17.558128Z","iopub.execute_input":"2024-04-21T08:47:17.558420Z","iopub.status.idle":"2024-04-21T08:47:19.372461Z","shell.execute_reply.started":"2024-04-21T08:47:17.558392Z","shell.execute_reply":"2024-04-21T08:47:19.371014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# median_income seems to have interesting correlation with median_house_value\nhousing.plot(kind=\"scatter\", x='median_income', y='median_house_value', alpha=0.2, grid=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.373956Z","iopub.execute_input":"2024-04-21T08:47:19.374487Z","iopub.status.idle":"2024-04-21T08:47:19.683928Z","shell.execute_reply.started":"2024-04-21T08:47:19.374456Z","shell.execute_reply":"2024-04-21T08:47:19.682084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### total rooms, total_bedrooms, population and households are not useful individually by themselves\n\nLet us create a new features based on these existing features","metadata":{}},{"cell_type":"code","source":"housing['rooms_per_house'] = housing['total_rooms'] / housing['households']\nhousing['bedrooms_ratio'] = housing['total_bedrooms'] / housing['total_rooms']\nhousing['people_per_house'] = housing['population'] / housing['households']\n\ncorr_matrix = housing.corr(numeric_only=True) # only checks for linear correlation... sometimes can be misleading\nprint(corr_matrix['median_house_value'].sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.685478Z","iopub.execute_input":"2024-04-21T08:47:19.686032Z","iopub.status.idle":"2024-04-21T08:47:19.710537Z","shell.execute_reply.started":"2024-04-21T08:47:19.685993Z","shell.execute_reply":"2024-04-21T08:47:19.708425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This much idea and general intuition about the data set should be good enough...\nLet us start with data processing for ML\n\n#### 1.3.4 Getting ready for ML\n\nGet X (features) and y (label/target)","metadata":{}},{"cell_type":"code","source":"housing = strat_train_set.drop('median_house_value', axis=1) # X\nhousing_labels = strat_train_set['median_house_value'].copy() # y","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.712283Z","iopub.execute_input":"2024-04-21T08:47:19.712990Z","iopub.status.idle":"2024-04-21T08:47:19.721057Z","shell.execute_reply.started":"2024-04-21T08:47:19.712951Z","shell.execute_reply":"2024-04-21T08:47:19.720114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us fix total_bedrooms having null values - Enter Imputer!\n\nWe can use median value to fill null values","metadata":{}},{"cell_type":"code","source":"# # Let us fix total_bedrooms having null values - Enter Imputer\n\n# from sklearn.impute import SimpleImputer # other imputers include KNNImputer, IterativeImputer\n# imputer = SimpleImputer(strategy='median') # other strategies are (\"mean\"), (\"most_frequent\"), (\"constant\", fill_value=...)\n\n# housing_num = housing.select_dtypes(include=[np.number]) # select only numeric columns\n# imputer.fit(housing_num)\n# # print(imputer.statistics_)\n# X = imputer.transform(housing_num)\n# # print(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.727753Z","iopub.execute_input":"2024-04-21T08:47:19.728565Z","iopub.status.idle":"2024-04-21T08:47:19.739148Z","shell.execute_reply.started":"2024-04-21T08:47:19.728483Z","shell.execute_reply":"2024-04-21T08:47:19.737627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # let us learn about handling categorical columns\n# housing_cat = housing[['ocean_proximity']]\n\n# # OrdinalEncoder - ordinal encoding might mislead ML algorithm by paying more attention than needed to the ordinal values\n# # OneHotEncoder is a better option (definitely preferred than pandas' get_dummies())\n# from sklearn.preprocessing import OneHotEncoder\n# cat_encoder = OneHotEncoder()\n# housing_cat_1hot = cat_encoder.fit_transform(housing_cat)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.741687Z","iopub.execute_input":"2024-04-21T08:47:19.742147Z","iopub.status.idle":"2024-04-21T08:47:19.761361Z","shell.execute_reply.started":"2024-04-21T08:47:19.742114Z","shell.execute_reply":"2024-04-21T08:47:19.759638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# std_scaler = StandardScaler()\n# housing_num_std_scaled = std_scaler.fit_transform(housing_num)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.763585Z","iopub.execute_input":"2024-04-21T08:47:19.765161Z","iopub.status.idle":"2024-04-21T08:47:19.777942Z","shell.execute_reply.started":"2024-04-21T08:47:19.765104Z","shell.execute_reply":"2024-04-21T08:47:19.776451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Feature scaling has more to learn like handling heavy tail then replacing them with its logarithm. Let us do that later...\n\n##### Sometimes even the target values need to be transformed (scaled) like using one's logarithm values\n##### then predicted values too would be logarithm values... inverse_tranform() would be useful to determine the actual/intended predicted value","metadata":{}},{"cell_type":"code","source":"# from sklearn.compose import TransformedTargetRegressor\n# from sklearn.linear_model import LinearRegression\n\n# model = TransformedTargetRegressor(LinearRegression(), transformer=StandardScaler())\n# model.fit(housing[['median_income']], housing_labels)\n# some_new_data = housing[[\"median_income\"]].iloc[:5]\n# predictions = model.predict(some_new_data)\n# print(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.779496Z","iopub.execute_input":"2024-04-21T08:47:19.779983Z","iopub.status.idle":"2024-04-21T08:47:19.792290Z","shell.execute_reply.started":"2024-04-21T08:47:19.779954Z","shell.execute_reply":"2024-04-21T08:47:19.791035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Let us visualize population","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nhousing[\"population\"].hist(ax=axs[0], bins=50)\nhousing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\naxs[0].set_xlabel(\"Population\")\naxs[1].set_xlabel(\"Log of population\")\naxs[0].set_ylabel(\"Number of districts\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:19.795327Z","iopub.execute_input":"2024-04-21T08:47:19.796374Z","iopub.status.idle":"2024-04-21T08:47:20.316994Z","shell.execute_reply.started":"2024-04-21T08:47:19.796331Z","shell.execute_reply":"2024-04-21T08:47:20.315528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n                              random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n    \n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:20.318647Z","iopub.execute_input":"2024-04-21T08:47:20.319032Z","iopub.status.idle":"2024-04-21T08:47:20.689109Z","shell.execute_reply.started":"2024-04-21T08:47:20.319008Z","shell.execute_reply":"2024-04-21T08:47:20.687204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_selector#, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n#                \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n# cat_attribs = [\"ocean_proximity\"]\n\n# num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(handle_unknown=\"ignore\"))\n\n# preprocessing = ColumnTransformer([\n#     (\"num\", num_pipeline, num_attribs),\n#     (\"cat\", cat_pipeline, cat_attribs),\n# ])\n\n# preprocessing = make_column_transformer(\n#                 (num_pipeline, make_column_selector(dtype_include=np.number)),\n#                 (cat_pipeline, make_column_selector(dtype_include=object)))\n\n# housing_prepared = preprocessing.fit_transform(housing)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:20.691457Z","iopub.execute_input":"2024-04-21T08:47:20.692464Z","iopub.status.idle":"2024-04-21T08:47:20.715838Z","shell.execute_reply.started":"2024-04-21T08:47:20.692415Z","shell.execute_reply":"2024-04-21T08:47:20.714186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return ['ratio']\n\ndef ratio_pipeline():\n    return make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n    StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_similarity = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessing = ColumnTransformer([\n    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n    (\"geo\", cluster_similarity, [\"latitude\", \"longitude\"]),\n    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),    \n], remainder=default_num_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:20.717681Z","iopub.execute_input":"2024-04-21T08:47:20.718187Z","iopub.status.idle":"2024-04-21T08:47:20.730763Z","shell.execute_reply.started":"2024-04-21T08:47:20.718149Z","shell.execute_reply":"2024-04-21T08:47:20.728978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinear_model = make_pipeline(preprocessing, LinearRegression())\nlinear_model.fit(housing, housing_labels)\n\nlinear_housing_predictions = linear_model.predict(housing)\n\nprint(linear_housing_predictions[:5].round(-2))\nprint(housing_labels.iloc[:5].values)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:20.732264Z","iopub.execute_input":"2024-04-21T08:47:20.732601Z","iopub.status.idle":"2024-04-21T08:47:21.888027Z","shell.execute_reply.started":"2024-04-21T08:47:20.732574Z","shell.execute_reply":"2024-04-21T08:47:21.887244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nlinear_rmse = mean_squared_error(housing_labels, linear_housing_predictions, squared=False)\nprint(linear_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:21.889611Z","iopub.execute_input":"2024-04-21T08:47:21.890779Z","iopub.status.idle":"2024-04-21T08:47:21.899003Z","shell.execute_reply.started":"2024-04-21T08:47:21.890752Z","shell.execute_reply":"2024-04-21T08:47:21.898259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_regressor = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_regressor.fit(housing, housing_labels)\n\ntree_housing_predictions = tree_regressor.predict(housing)\n\ntree_rmse = mean_squared_error(housing_labels, tree_housing_predictions, squared=False)\nprint(tree_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:21.900454Z","iopub.execute_input":"2024-04-21T08:47:21.901484Z","iopub.status.idle":"2024-04-21T08:47:23.433651Z","shell.execute_reply.started":"2024-04-21T08:47:21.901458Z","shell.execute_reply":"2024-04-21T08:47:23.432856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ntree_rmses = -1 * cross_val_score(tree_regressor, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\nprint(pd.Series(tree_rmses).describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:23.435003Z","iopub.execute_input":"2024-04-21T08:47:23.435504Z","iopub.status.idle":"2024-04-21T08:47:40.663682Z","shell.execute_reply.started":"2024-04-21T08:47:23.435471Z","shell.execute_reply":"2024-04-21T08:47:40.662632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_regressor = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))\nrandomforest_rmses = -1 * cross_val_score(forest_regressor, housing, housing_labels, scoring='neg_root_mean_squared_error', cv=10)\nprint(pd.Series(tree_rmses).describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:47:40.664996Z","iopub.execute_input":"2024-04-21T08:47:40.665288Z","iopub.status.idle":"2024-04-21T08:51:37.311066Z","shell.execute_reply.started":"2024-04-21T08:47:40.665259Z","shell.execute_reply":"2024-04-21T08:51:37.310062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nfull_pipeline = Pipeline([('preprocessing', preprocessing),\n                         ('random_forest', RandomForestRegressor(random_state=42))])\n\nparam_grid = [{'preprocessing__geo__n_clusters': [5, 8, 10], 'random_forest__max_features': [4, 6, 8]}, # 3 * 3 = 9\n             {'preprocessing__geo__n_clusters': [10, 15], 'random_forest__max_features': [6, 8, 10]}] # 2 * 3 = 6\n\n# (9 + 6) * cv (3) = 45 rounds of training\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\ngrid_search.fit(housing, housing_labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:52:03.999129Z","iopub.execute_input":"2024-04-21T08:52:03.999666Z","iopub.status.idle":"2024-04-21T08:57:01.680117Z","shell.execute_reply.started":"2024-04-21T08:52:03.999633Z","shell.execute_reply":"2024-04-21T08:57:01.677905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search.best_params_)\ncv_res = pd.DataFrame(grid_search.cv_results_)\ncv_res.sort_values(by='mean_test_score', ascending=False, inplace=True)\nprint(cv_res.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:57:50.146619Z","iopub.execute_input":"2024-04-21T08:57:50.147046Z","iopub.status.idle":"2024-04-21T08:57:50.166652Z","shell.execute_reply.started":"2024-04-21T08:57:50.147018Z","shell.execute_reply":"2024-04-21T08:57:50.165154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n                 'random_forest__max_features': randint(low=2, high=20)}\n\nrandom_search = RandomizedSearchCV(full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n                                   scoring='neg_root_mean_squared_error', random_state=42)\n\nrandom_search.fit(housing, housing_labels)\n\n# finally (hopefully!)\nfinal_model = random_search.best_estimator_\nfeature_importances = final_model['random_forest'].feature_importances_\nprint(feature_importances.round(2))\n\nprint(sorted(zip(feature_importances, final_model['preprocessing'].get_feature_names_out()), reverse=True))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:59:10.999211Z","iopub.execute_input":"2024-04-21T08:59:10.999582Z","iopub.status.idle":"2024-04-21T09:03:05.392061Z","shell.execute_reply.started":"2024-04-21T08:59:10.999555Z","shell.execute_reply":"2024-04-21T09:03:05.390791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set['median_house_value'].copy()\n\nfinal_predictions = final_model.predict(X_test)\nfinal_rmse = mean_squared_error(y_test, final_predictions, squared=False)\nprint(final_rmse)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T09:04:15.261495Z","iopub.execute_input":"2024-04-21T09:04:15.261873Z","iopub.status.idle":"2024-04-21T09:04:15.425675Z","shell.execute_reply.started":"2024-04-21T09:04:15.261846Z","shell.execute_reply":"2024-04-21T09:04:15.424819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T09:04:59.421529Z","iopub.execute_input":"2024-04-21T09:04:59.421939Z","iopub.status.idle":"2024-04-21T09:04:59.437773Z","shell.execute_reply.started":"2024-04-21T09:04:59.421893Z","shell.execute_reply":"2024-04-21T09:04:59.435473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\njoblib.dump(final_model, 'california_housing_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T09:11:04.157106Z","iopub.execute_input":"2024-04-21T09:11:04.157514Z","iopub.status.idle":"2024-04-21T09:11:04.323362Z","shell.execute_reply.started":"2024-04-21T09:11:04.157479Z","shell.execute_reply":"2024-04-21T09:11:04.322641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra code – excluded for conciseness\n# from sklearn.cluster import KMeans\n# from sklearn.base import BaseEstimator, TransformerMixin\n# from sklearn.metrics.pairwise import rbf_kernel\n\n# def column_ratio(X):\n#     return X[:, [0]] / X[:, [1]]\n\n# #class ClusterSimilarity(BaseEstimator, TransformerMixin):\n# #    [...]\n\n# final_model_reloaded = joblib.load(\"california_housing_model.pkl\")\n\n# new_data = housing.iloc[:5]  # pretend these are new districts\n# predictions = final_model_reloaded.predict(new_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T09:11:42.005500Z","iopub.execute_input":"2024-04-21T09:11:42.006167Z","iopub.status.idle":"2024-04-21T09:11:42.247385Z","shell.execute_reply.started":"2024-04-21T09:11:42.006113Z","shell.execute_reply":"2024-04-21T09:11:42.245981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-21T09:11:49.771362Z","iopub.execute_input":"2024-04-21T09:11:49.771841Z","iopub.status.idle":"2024-04-21T09:11:49.780927Z","shell.execute_reply.started":"2024-04-21T09:11:49.771806Z","shell.execute_reply":"2024-04-21T09:11:49.779165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}